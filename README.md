# üèóÔ∏è Project Architecture Overview

## üì¶ Modules

- efficientNet_model/ ‚Äî EfficientNet classifier for detected items (developed by Karim)
- yolo_model/ ‚Äî YOLOv8n object detector (developed by Daulet)
- backend/ ‚Äî FastAPI server (developed by Stefan)
- app/ ‚Äî Frontend/mobile interface (developed by Asylkhan)
- docs/ ‚Äî Documentation folder (developed by Asylkhan)

## üß† Workflow
1. User uploads or captures image in the mobile app
2. Image sent to backend via FastAPI
3. Depending on mode:
   - YOLO detects multiple objects
   - EfficientNet classifies detected objects one by one.
4. Prediction returned to frontend

## üì• Download
1. All the requirements and libraries for the project are in the **requirements.txt** (no react_native libraries)
2. Model weights for EfficientNet(MaybeTheBest2.pth) and Yolo(bestYolo2.pt) are available here: [Google Drive Link](https://drive.google.com/drive/folders/1zjOuYtXwM5OrVcitNX3mJ4vauTer9j85?usp=share_link)
3. Demo Video of Mobile App [Google Drive Link](https://drive.google.com/file/d/1s14zmmTMxAKD6HVjtnieNHRnDSzfx_MK/view?usp=share_link)

## üôå Team Contributions
- EfficientNet model: Karim
- YOLO model: Daulet
- Backend/API: Stefan
- Mobile App: Asylkhan
- Documentation: Asylkhan
---
---

# EfficientNetV2-S Fine-Tuning for Image Classification

This repository contains a training pipeline for **image classification** using **EfficientNetV2-S** with fine-tuning.  
The code supports **multi-stage training** (head ‚Üí last block ‚Üí pre-last block), **class balancing**, **advanced augmentations**, and **early stopping** based on macro-F1 score.

---

## üöÄ Features
- **Pretrained EfficientNetV2-S** (ImageNet weights)
- **Custom classifier head** with BatchNorm, Dropout, and Linear layers
- **Stage-wise fine-tuning**:
  - Stage 1: train only the classifier head
  - Stage 2: train head + last feature block
  - (Optional Stage 3: head + last two feature blocks)
- **WeightedRandomSampler** for imbalanced datasets
- **OneCycleLR scheduler** with different learning rates per parameter group
- **Macro-F1 evaluation** and early stopping
- **Heavy augmentations** for better generalization (rotation, affine, perspective, blur, jitter, erasing, etc.)

---

## üìÇ Dataset Structure
Dataset should be organized in **ImageFolder format**:
```
Cropped_images_dataset/
‚îÇ‚îÄ‚îÄ class_0/(cardboard),
‚îÇ‚îÄ‚îÄ class_1/(glass),
‚îÇ‚îÄ‚îÄ class_2/(metal),
|‚îÄ‚îÄ class_3/(paper),
|‚îÄ‚îÄ class_4/(plastic),
‚îÇ‚îÄ‚îÄ class_5/(trash).
```

## ‚öôÔ∏è Key Parameters
```
Input size: 288x288, Crop size: 256x256
Loss: CrossEntropyLoss with class weights + label smoothing (0.02)
Optimizer: AdamW
Scheduler: OneCycleLR
Early stopping: patience=10, delta=0.001
```

## üìå Requirements
```
Python 3.9+
PyTorch 2.0+
torchvision
scikit-learn
numpy, pillow
```
# üìä Evaluation
```
Evaluate accuracy and macro-F1:

get_accuracy(model, test_loader)
eval_macro_f1(model, test_loader, class_names=classes, device=device, verbose=True)
```

## üìä Results
- **Approximate Accuracy**: ~85%  
  *(classification accuracy on detected objects)*  
  ‚ö†Ô∏è *This is an estimated value provided by the author ‚Äî validation is still pending.*

---

## üë§ Author

- **Model trained by**: Karim
- **Documentation prepared by**: Karim
------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------

# üß† YOLOv8 Model for Garbage Detection

## üìå Overview
This document describes the training and usage of a YOLOv8n object detection model designed for garbage classification.

- **Purpose**: Detect garbage items across 6 categories.
- **Model Used**: YOLOv8n (nano version)
- **Why YOLOv8n?** Lightweight, fast, suitable for real-time detection on mobile or embedded devices.
- **Training Source**: Dataset located at `/kaggle/input/garbage-detection/GARBAGE CLASSIFICATION/data.yaml`

---

## üß† Model Architecture
- **Base Model**: `yolov8n.pt`
- **Transfer Learning**: Yes (pretrained weights used)
- **Number of Classes**: 6
- **Class Labels**:
  - BIODEGRADABLE
  - CARDBOARD
  - GLASS
  - METAL
  - PAPER
  - PLASTIC

---

## üß™ Training Details
- **Epochs**: 120  
- **Batch Size**: 32  
- **Image Size**: 640√ó640  
- **Learning Rate (initial)**: 0.01  
- **Weight Decay**: 0.0005  
- **Warmup Epochs**: 3.0  
- **Momentum**: 0.937  
- **Augmentations**:
  - Horizontal Flip: 0.5  
  - Translate: 0.1  
  - Scale: 0.5  
  - HSV Adjustments: Hue = 0.015, Saturation = 0.7, Value = 0.4  
  - Random Erasing: 0.4  
  - Mosaic: Enabled  
  - AutoAugment: RandAugment  
- **Optimizer**: Automatically selected by Ultralytics  
- **Pretrained**: True

---

## üìä Results
- **Approximate Accuracy**: ~75%  
  *(Bounding box detection)*  
  ‚ö†Ô∏è *This is an estimated value provided by the author ‚Äî validation is still pending.*

---

## üì• How Prediction Works

This model is used in **object detection mode** to identify multiple types of garbage items in a single image.

### üîÑ Prediction Flow:
1. Input image is resized to **640√ó640**
2. Image is passed through the YOLOv8 detection engine
3. Model returns:
   - üì¶ Bounding boxes (object locations)
   - üìà Confidence scores for each prediction

---

## üë§ Author

- **Model trained by**: Daulet  
- **Documentation prepared by**: Asylkhan

---
---
---

# Garbage Detection AI Server Documentation

 ## 1. Overview
 This document describes the functionality of the main component which serves the purpose of accepting client requests, processing them and returning the desired result in an efficient manner.  The server is composed of **3 parts**:
 

 - **API Layer,** which ensures connectivity between the clients and the processing layer
 - **Processing Layer,** which parses client requests and runs inference on the data received, sending back inference results
 - **Asset Collector,** which makes sure that model files are present and accesible for the processing layer

## 2. API Layer
By making use of the **FastAPI** framework, the API layer exposes an endpoint where the client would send the image data and receive the result of the inference process (detection label, bounding boxes).  
The API layer class instantiates the **classifier object** and passes the data received from the client for further processing by placing it in a queue and awaiting the results using asynchronous methods.

## 3. Processing Layer
This component is responsible for emptying the queue containing client data, running inference, and returning the results. A worker method constantly checks if the queue contains data, in which case it will pass the image data to the `run_inference` method where the detection models will be employed.  
The `run_inference` method is called at the API layer in order to obtain the results from the classifier class, passing them back to the client in JSON format.

## 4. Asset Collector
The asset collector object will be instantiated at the processing layer level in order to ensure the server has the necessary files for running inference (e.g. model files, post processing files) and to restrict the access to those files outside the scope of the server. By making use of custom exceptions and proper handling, the server will not start if model files are not present where they are supposed to be. The asset collector object encapsulates the paths to the models, making the accesible only at the processing layer level, therefore keeping sensitive information away from the clients.

---
---
---

# Documentation for ‚Å† App.js ‚Å† and running the app (Expo + real server)
>
	‚Å†In short: the React Native (Expo) client takes/makes a photo, shows a preview, *sends it to a real backend* and displays the *actual result*. In this version, the API address is set as a **constant in ‚Å† App.js ‚Å†*:

 ```js
 const API_URL = 'http://<PC_IPV4>:8000/classify-image';
  ```
>
	‚Å†Replace ‚Å† <PC_IPV4> ‚Å† with the local IPv4 address of your computer, which must be on the same Wi-Fi network as your phone.

---

## 1) Structure of ‚Å† App.js ‚Å†

*Purpose: the root component. Initializes camera/gallery permissions, renders the UI, prepares bytes (if needed), **sends the image to ‚Å† API_URL ‚Å†* and displays the response.*

### 1.1 Imports (typical)

‚Ä¢‚Å†  ‚Å†‚Å† react ‚Å†, ‚Å† useState ‚Å†, ‚Å† useEffect ‚Å†, ‚Å† useMemo ‚Å†, ‚Å† useCallback ‚Å†  
‚Ä¢‚Å†  ‚Å†‚Å† expo-image-picker ‚Å† (gallery/camera)  
‚Ä¢‚Å†  ‚Å†‚Å† react-native ‚Å† ‚Äî ‚Å† View ‚Å†, ‚Å† Text ‚Å†, ‚Å† Image ‚Å†, ‚Å† Pressable ‚Å†, ‚Å† SafeAreaView ‚Å†, ‚Å† StyleSheet ‚Å†, ‚Å† ActivityIndicator ‚Å†, ‚Å† Alert ‚Å†  
‚Ä¢‚Å†  ‚Å†‚Å† expo-file-system/legacy ‚Å† ‚Äî read file from filesystem  
‚Ä¢‚Å†  ‚Å†‚Å† base64-js ‚Å† ‚Äî reliable conversion bytes ‚ÜîÔ∏è base64  

### 1.2 State

‚Ä¢‚Å†  ‚Å†‚Å† imageUri ‚Å† ‚Äî path to the selected/captured file  
‚Ä¢‚Å†  ‚Å†‚Å† phase ‚Å† ‚Äî screen stage: ‚Å† CAPTURE ‚Å† ‚Üí ‚Å† PREVIEW ‚Å† ‚Üí ‚Å† PROCESSING ‚Å† ‚Üí ‚Å† RESULT ‚Å†  
‚Ä¢‚Å†  ‚Å†‚Å† loading ‚Å† ‚Äî request indicator  
‚Ä¢‚Å†  ‚Å†‚Å† result ‚Å† ‚Äî response data (image/JSON depending on endpoint)  

### 1.3 Key functions

‚Ä¢‚Å†  ‚Å†‚Å† pickImage() ‚Å† / ‚Å† takePhoto() ‚Å† ‚Äî select/capture and move to preview  
‚Ä¢‚Å†  ‚Å†‚Å† readBytes(uri) ‚Å† ‚Äî read file and prepare *‚Å† Uint8Array ‚Å†* or *base64* (both options exist in the project)  
‚Ä¢‚Å†  ‚Å†‚Å† sendToServer() ‚Å† ‚Äî send to *‚Å† API_URL ‚Å†. In this build, the path *‚Å† /classify-image ‚Å†* is used, which returns an **image* (binary/octet-stream or image/*). Alternative endpoint ‚Äî ‚Å† /classify ‚Å† with JSON (if enabled on the server)  
‚Ä¢‚Å†  ‚Å†‚Å† reset() ‚Å† ‚Äî clear state for a new capture  

### 1.4 UI

‚Ä¢‚Å†  ‚Å†Buttons: ‚ÄúPick from Gallery‚Äù, ‚ÄúTake Photo‚Äù, ‚ÄúSend‚Äù, ‚ÄúRetry‚Äù  
‚Ä¢‚Å†  ‚Å†Preview screen with mini-cards/color theme  
‚Ä¢‚Å†  ‚Å†Processing screen with loader  
‚Ä¢‚Å†  ‚Å†Result screen: response image *or* JSON fields ‚Äî depending on the chosen endpoint  

---

## 2) API address configuration

At the top of ‚Å† App.js ‚Å† find the line:
```‚Å†js
const API_URL = "http://10.9.105.98:8000/classify-image";
 ```

and replace the IP *with your PC‚Äôs address* in the local network (example: ‚Å† 192.168.0.12 ‚Å†). Make sure your phone and PC are connected to the *same* Wi-Fi network.

	‚Å†How to check IPv4 on Windows: ‚Å† Win + R ‚Å† ‚Üí ‚Å† cmd ‚Å† ‚Üí ‚Å† ipconfig ‚Å† ‚Üí line ‚Å† IPv4 Address ‚Å†. Take the address of your active network (usually 192.168.x.x).

If you need environment switching without editing code, you can replace the constant with reading from ‚Å† .env ‚Å† (‚Å† EXPO_PUBLIC_API_URL ‚Å†) ‚Äî but the current version uses a *hard-coded* ‚Å† API_URL ‚Å†.

---

## 3) Sending to the server: two scenarios

### 3.1 Server returns an *image* (‚Å† /classify-image ‚Å†)

```js
async function sendToServer() {
  try {
    setLoading(true);

    // prepare form-data with file
    const form = new FormData();
    form.append('file', { uri: imageUri, name: 'image.jpg', type: 'image/jpeg' });

    const res = await fetch(API_URL, { method: 'POST', body: form });
    if (!res.ok) throw new Error(`HTTP ${res.status}`);

    // read binary response and show as image
    const blob = await res.blob();
    const reader = new FileReader();
    reader.onloadend = () => {
      // data URL: can be passed into <Image source={{ uri: reader.result }} />
      setResult({ previewUri: reader.result, type: 'image' });
    };
    reader.readAsDataURL(blob);
  } catch (e) {
    Alert.alert('Error', e?.message || 'Failed to send image');
  } finally {
    setLoading(false);
  }
}
```

	‚Å†In Expo (native) instead of ‚Å† FileReader ‚Å† you can use ‚Å† expo-file-system ‚Å† / ‚Å† ImageManipulator ‚Å†, or save blob to a file and provide ‚Å† uri ‚Å† to the ‚Å† <Image/> ‚Å† component.

### 3.2 Server returns *JSON* (‚Å† /classify ‚Å†)

```js
const API_JSON = 'http://<PC_IPV4>:8000/classify';

async function sendToServerJson() {
  setLoading(true);
  try {
    const form = new FormData();
    form.append('file', { uri: imageUri, name: 'image.jpg', type: 'image/jpeg' });

    const res = await fetch(API_JSON, { method: 'POST', body: form, headers: { Accept: 'application/json' } });
    if (!res.ok) throw new Error(`HTTP ${res.status}`);

    const json = await res.json();
    setResult({ ...json, type: 'json' }); // expected: { label, score, ... }
  } catch (e) {
    Alert.alert('Error', e?.message || 'Request failed');
  } finally {
    setLoading(false);
  }
}
```

---

## 4) Quick start (Frontend, Expo)

‚Å† bash
# install dependencies
npm install
# or
yarn install

# start Metro
npx expo start
 ‚Å†

Then open *Expo Go* on your phone ‚Üí scan the QR code. If LAN does not work, switch to *Tunnel* in the Expo web panel.

*Common issues*: cache (‚Å† npx expo start -c ‚Å†), camera/gallery permissions, VPN/firewall, router client isolation (AP isolation), connect to the same WiFi with computer.

---

## 5) Server (for reference)

Your real server is already running. If you need a local FastAPI mock ‚Äî draft below:

```python
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse, StreamingResponse
from io import BytesIO

app = FastAPI()

@app.post('/classify')
async def classify(file: UploadFile = File(...)):
    data = await file.read()
    return JSONResponse({'label': 'demo', 'score': 1.0, 'bytes': len(data)})

@app.post('/classify-image')
async def classify_image(file: UploadFile = File(...)):
    data = await file.read()
    return StreamingResponse(BytesIO(data), media_type=file.content_type)
```

Run with: ‚Å† uvicorn main:app --reload --host 0.0.0.0 --port 8000 ‚Å†.

---

## 6) Report checklist

‚Ä¢‚Å†  ‚Å†[ ] QR code ‚Å† npx expo start ‚Å† in terminal/browser  
‚Ä¢‚Å†  ‚Å†[ ] Source selection screen in Expo Go  
‚Ä¢‚Å†  ‚Å†[ ] File preview screen  
‚Ä¢‚Å†  ‚Å†[ ] Result screen:

  * image response for ‚Å† /classify-image ‚Å†, *or*  
  * JSON fields (label/score/...) for ‚Å† /classify ‚Å†  
‚Ä¢‚Å†  ‚Å†[ ] (Optional) screenshot of request to ‚Å† http://<PC_IPV4>:8000/classify-image ‚Å† or ‚Å† /classify ‚Å†  

---

## 7) Short defense

*EN (ref): The Expo client handles image selection/capture, sends the file to a **live endpoint* ‚Å† API_URL ‚Å† (‚Å† /classify-image ‚Å† by default) and renders either an image or JSON response. The API address is hard-coded in ‚Å† App.js ‚Å† for fast LAN testing.*


